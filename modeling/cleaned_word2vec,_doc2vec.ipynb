{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9lHoIdS2o83"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/tr_q1_pair_with_similarity_cos.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = data[[\"right_ans\",\"student_ans\"]]\n",
        "data2"
      ],
      "metadata": {
        "id": "NWiIP3QS20vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def make_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "data2['cleaned'] = data2['student_ans'].apply(make_lower_case)\n",
        "data2['cleaned'] = data2.cleaned.apply(remove_stop_words)\n",
        "data2['cleaned'] = data2.cleaned.apply(remove_html)\n"
      ],
      "metadata": {
        "id": "WsbRrf76NBlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2"
      ],
      "metadata": {
        "id": "5AVKECLqPA0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "senlist = []\n",
        "for sentence in data2[\"cleaned\"]:\n",
        "  a = sentence.replace(\"\\ufeff\", \"\")\n",
        "  senlist.append(a)"
      ],
      "metadata": {
        "id": "yyhjOkfj3CvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "senlist"
      ],
      "metadata": {
        "id": "7ICRqXFf4MtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "commentlist=[]\n",
        "\n",
        "for sentence in data2[\"student_ans\"]:\n",
        "\n",
        "  word_tokens = word_tokenize(sentence)\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for w in word_tokens:\n",
        "\n",
        "    result.append(w)\n",
        "\n",
        "  commentlist.append(result)\n",
        "\n",
        "commentlist"
      ],
      "metadata": {
        "id": "6ugjFoDK4Ox-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commentlist_cleaned=[]\n",
        "\n",
        "for sentence2 in data2[\"cleaned\"]:\n",
        "\n",
        "  word_tokens2 = word_tokenize(sentence2)\n",
        "\n",
        "  result_cleaned = []\n",
        "\n",
        "  for w2 in word_tokens2:\n",
        "\n",
        "    result_cleaned.append(w2)\n",
        "\n",
        "  commentlist_cleaned.append(result_cleaned)\n",
        "\n",
        "commentlist_cleaned"
      ],
      "metadata": {
        "id": "GD1s6WH_P2qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences=commentlist, size=100, window=5, min_count=0, workers=4, sg=0)\n",
        "model_cleaned = Word2Vec(sentences=commentlist_cleaned, size=100, window=5, min_count=0, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "ilUqaw2R5ZxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.wv.most_similar(\"parallel\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "id": "xG4VHIeR5_De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cleaned_result = model_cleaned.wv.most_similar(\"parallel\")\n",
        "print(model_cleaned_result)"
      ],
      "metadata": {
        "id": "mTRKZP1a62XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어벡터의 평균 구하기"
      ],
      "metadata": {
        "id": "Fq5qV1P1Sb5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_vectors(document_list):\n",
        "    document_embedding_list = []\n",
        "\n",
        "    # 각 문서에 대해서\n",
        "    for line in document_list:\n",
        "        doc2vec = None\n",
        "        count = 0\n",
        "        for word in line.split():\n",
        "            if word in model_cleaned.wv.vocab:\n",
        "                count += 1\n",
        "                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.\n",
        "                if doc2vec is None:\n",
        "                    doc2vec = model_cleaned[word]\n",
        "                else:\n",
        "                    doc2vec = doc2vec + model_cleaned[word]\n",
        "\n",
        "        if doc2vec is not None:\n",
        "            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.\n",
        "            doc2vec = doc2vec / count\n",
        "            document_embedding_list.append(doc2vec)\n",
        "\n",
        "    # 각 문서에 대한 문서 벡터 리스트를 리턴\n",
        "    return document_embedding_list"
      ],
      "metadata": {
        "id": "_2D_uRV8R6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_document_vectors(data2['cleaned'])"
      ],
      "metadata": {
        "id": "S_rHgCPGSva_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8ebVdrMS13d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}