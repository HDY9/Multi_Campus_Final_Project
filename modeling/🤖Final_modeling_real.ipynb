{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJW9TGC1-ET_"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install urllib3\n",
        "!pip install language-tool-python "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import language_tool_python\n",
        "from multiprocessing import Process"
      ],
      "metadata": {
        "id": "P99f3GIe-VEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-QdJmLm8brv"
      },
      "source": [
        "# SBERT\n",
        "## Similarity_Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au7l-u5h8SD6"
      },
      "outputs": [],
      "source": [
        "df_right_answers = pd.read_csv('/content/drive/MyDrive/final project/Dataset/input_data/right_ans/EN_right_ans.csv',index_col=0)\n",
        "df_right_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLldjJKo8uaP"
      },
      "outputs": [],
      "source": [
        "wai_path = '/content/drive/MyDrive/final project/Dataset/input_data'  # for_valid 폴더 생성  \n",
        "wai_questions = os.listdir(wai_path)\n",
        "wai_questions = [p for p in wai_questions if 'csv' in p]\n",
        "wai_questions  # 컬럼명 질문 내용 답변인 원본 데이터 프레임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp4634569Ikx"
      },
      "source": [
        "### 테스트용 문장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd_yUK0K8uc0"
      },
      "outputs": [],
      "source": [
        "sentences = ['You are so beautiful', 'You are so kind', 'You are too bad', 'You are too noisy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpZ4oVDR9Ort"
      },
      "source": [
        "### Sentence Embedding\n",
        "- 다양한 사전 학습 모델을 통해 문장 embedding 결과\\\n",
        " \"xlm-r-large-en-ko-nli-ststb\" 가 가장 적합하다 판단"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ml_L6 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings_L6 = model_ml_L6.encode(sentences)\n",
        "sentences, embeddings_L6"
      ],
      "metadata": {
        "id": "5vk5CUAQ8aPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ml_all = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "embeddings_all = model_ml_all.encode(sentences)\n",
        "sentences, embeddings_all"
      ],
      "metadata": {
        "id": "pau76Bum7dQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kns5q_Ri9TxC"
      },
      "outputs": [],
      "source": [
        "model_ml = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "embeddings = model_ml.encode(sentences)\n",
        "sentences, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI56mx2d9aip"
      },
      "outputs": [],
      "source": [
        "sbert_EN = SentenceTransformer('sentence-transformers/xlm-r-large-en-ko-nli-ststb')\n",
        "embeddings_EN = sbert_EN.encode(sentences)\n",
        "embeddings_EN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU7V8kCy9em6"
      },
      "source": [
        "## 유사도 구하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hiun2wO9iew"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "for i, emb in enumerate(embeddings_EN):\n",
        "    for j in range(i+1, len(embeddings_EN)):\n",
        "        pairs.append((sentences[i], sentences[j],emb, embeddings_EN[j]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yB-lHby9ihG"
      },
      "outputs": [],
      "source": [
        "def get_similarity(ans, right_ans, use=\"cosine\"):\n",
        "    # Cosine Similarity\n",
        "    if use == \"cosine\":\n",
        "        return dot(ans, right_ans)/(norm(ans)*norm(right_ans))\n",
        "        \n",
        "    # # Euclidean\n",
        "    # if use == \"euclidean\":\n",
        "    #     if norm(ans-right_ans)==norm(ans-right_ans):\n",
        "    #         return norm(ans-right_ans)\n",
        "    #     else:\n",
        "    #         return -1\n",
        "    \n",
        "    \n",
        "    # # Pearson\n",
        "    # if use == \"pearson\":\n",
        "    #     return dot((ans - np.mean(ans)), (right_ans - np.mean(right_ans))) / ((norm(ans - np.mean(ans))) * (norm(right_ans - np.mean(right_ans))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0YmsDRO9ij7"
      },
      "outputs": [],
      "source": [
        "for (sent1,sent2, ans, r_ans) in pairs:\n",
        "    print(sent1,\" // \", sent2, get_similarity(ans, r_ans))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xXk_8s96rb"
      },
      "source": [
        "## Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGz_ws5mfzp9"
      },
      "outputs": [],
      "source": [
        "pass_list = ['cosine_similarity','grammar_checker','keyword_sum','total_score']\n",
        "\n",
        "for path in pass_list:\n",
        "    os.makedirs(\"/content/drive/MyDrive/final project/Dataset/output_data/\"+path,  exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ukaw5G-Ac0"
      },
      "outputs": [],
      "source": [
        "def sbert_final(pathname):\n",
        "  for q in wai_questions:\n",
        "      q_pd = pd.read_csv(os.path.join(pathname,q),index_col=0)\n",
        "      \n",
        "      start = time.time()  # 시작 시간 저장\n",
        "      \n",
        "      # # 전처리\n",
        "      # question = re.sub(r\"[^a-zA-Z가-힣\\s\\t\\?\\']\", \"\", q_pd.columns[0])\n",
        "      # print(question)\n",
        "      \n",
        "      # answers = list(str(q_pd[q_pd.columns[0]]))\n",
        "      answers=[]\n",
        "      for i in range(len(q_pd[q_pd.columns[0]])):\n",
        "        a=str(q_pd[q_pd.columns[0]][i])\n",
        "        answers.append(a)\n",
        "      # answers = [re.sub(r\"[^a-zA-Z가-힣\\s\\t\\.\\']\", \"\", str(ans)) for ans in answers if ans==ans]\n",
        "      # answers = [ans for ans in answers if ans!=\"\"]\n",
        "      \n",
        "      # print(f\"전처리 시간 : {time.time() - start:.3f} 초\")  # 현재시각 - 시작시간 = 실행 시간\n",
        "      # print()\n",
        "      # start = time.time()\n",
        "\n",
        "      # new_preprocessing\n",
        "      # answer = []\n",
        "      # for ans in answers:\n",
        "      #   if ans == ans:\n",
        "      #     a = re.sub(r\"[^a-zA-Z가-힣\\s\\t\\.\\']\", \"\", str(ans))\n",
        "      #     answer.append(a)\n",
        "      \n",
        "      # 한 답변씩 유사도 검사\n",
        "      print(q.split('_')[2].split('.')[0])\n",
        "      print(df_right_answers['question_num']==\"q\"+q.split('_')[2].split('.')[0])\n",
        "\n",
        "      right_answers = list(df_right_answers[df_right_answers['question_num']==\"q\"+q.split('_')[2].split('.')[0]]['right_ans'])\n",
        "      right_answers_emb = sbert_EN.encode(right_answers)\n",
        "      \n",
        "      print(f\"모범답안 embedding 시간 : {time.time() - start:.3f} 초\")  # 현재시각 - 시작시간 = 실행 시간\n",
        "      print()\n",
        "      start = time.time()\n",
        "      \n",
        "      \n",
        "      similarity_cos = []\n",
        "      similarity_euclidean = []\n",
        "      similarity_pearson = []\n",
        "      similarity_total = []\n",
        "      for i in tqdm(range(len(answers))):\n",
        "          ans = answers[i]\n",
        "          ans_emb = sbert_EN.encode([ans])\n",
        "          for j in range(len(right_answers_emb)):\n",
        "              r_emb = right_answers_emb[j]\n",
        "              cos_sim = get_similarity(ans_emb[0], r_emb, use='cosine')\n",
        "              # euclidean_sim = get_similarity(ans_emb[0], r_emb, use='euclidean')\n",
        "              # pearson_sim = get_similarity(ans_emb[0], r_emb, use='pearson')\n",
        "              \n",
        "              similarity_cos.append((right_answers[j], ans, cos_sim))\n",
        "              # similarity_euclidean.append((right_answers[j], ans, euclidean_sim))\n",
        "              # similarity_pearson.append((right_answers[j], ans, pearson_sim))\n",
        "              # similarity_total.append((right_answers[j], ans, cos_sim, euclidean_sim, pearson_sim))\n",
        "      print(ans)\n",
        "      # print(f\"학생 답안 embedding & 유사도 검사 시간 : {time.time() - start:.3f} 초\")  # 현재시각 - 시작시간 = 실행 시간\n",
        "      print()\n",
        "      start = time.time()\n",
        "      \n",
        "      \n",
        "      # similarity 기준 sort\n",
        "      similarity_cos_sorted = sorted(similarity_cos, key = lambda x : x[2], reverse=True)\n",
        "      # similarity_euclidean_sorted = sorted(similarity_euclidean, key = lambda x : x[2])\n",
        "      # similarity_pearson_sorted = sorted(similarity_pearson, key = lambda x : x[2], reverse=True)\n",
        "      # similarity_total_sorted = sorted(similarity_total, key = lambda x : x[2], reverse=True)\n",
        "\n",
        "      print(similarity_cos_sorted)\n",
        "\n",
        "      # csv 저장\n",
        "      df_similarity_cos = pd.DataFrame({'right_ans': [x[0] for x in similarity_cos_sorted],\n",
        "                              'student_ans':[x[1] for x in similarity_cos_sorted],\n",
        "                              'similarity':[x[2] for x in similarity_cos_sorted]})\n",
        "      \n",
        "      df_similarity_cos.to_csv(f\"/content/drive/MyDrive/final project/Dataset/output_data/cosine_similarity/similarity_cos_q{q.split('_')[2].split('.')[0]}.csv\")\n",
        "\n",
        "      # df_similarity_euclidean = pd.DataFrame({'right_ans': [x[0] for x in similarity_euclidean_sorted],\n",
        "      #                         'student_ans':[x[1] for x in similarity_euclidean_sorted],\n",
        "      #                         'similarity':[x[2] for x in similarity_euclidean_sorted]})\n",
        "      # df_similarity_euclidean.to_csv(f\"/content/drive/MyDrive/final project/Dataset/euclidian_similarity/{q.split('.')[0]}_pair_with_similarity_eu.csv\")\n",
        "\n",
        "      # df_similarity_pearson = pd.DataFrame({'right_ans': [x[0] for x in similarity_pearson_sorted],\n",
        "      #                         'student_ans':[x[1] for x in similarity_pearson_sorted],\n",
        "      #                         'similarity':[x[2] for x in similarity_pearson_sorted]})\n",
        "      # df_similarity_pearson.to_csv(f\"/content/drive/MyDrive/final project/Dataset/pearson_similarity/{q.split('.')[0]}_pair_with_similarity_pearson.csv\")\n",
        "\n",
        "      # df_similarity_total = pd.DataFrame({'right_ans': [x[0] for x in similarity_total_sorted],\n",
        "      #                         'student_ans':[x[1] for x in similarity_total_sorted],\n",
        "      #                         'cos_similarity':[x[2] for x in similarity_total_sorted],\n",
        "      #                         'euclidean_similarity':[x[3] for x in similarity_total_sorted],\n",
        "      #                         'pearson_similarity':[x[4] for x in similarity_total_sorted]})\n",
        "      # df_similarity_total.to_csv(f\"/content/drive/MyDrive/final project/Dataset/whole_similarity/{q.split('.')[0]}_pair_with_similarity_total.csv\")\n",
        "      print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPmeyXO2jfZQ"
      },
      "outputs": [],
      "source": [
        "sbert_final(wai_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrU05uO_nNE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKL3vmaE_TAf"
      },
      "source": [
        "# Grammer Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnd7DbuF5JSb"
      },
      "outputs": [],
      "source": [
        "cos_path = '/content/drive/MyDrive/final project/Dataset/output_data/cosine_similarity/'   \n",
        "cos_file = os.listdir(cos_path)\n",
        "cos_file = [p for p in cos_file if 'csv' in p]\n",
        "print(len(cos_file))  # 컬럼명 질문 내용 답변인 원본 데이터 프레임\n",
        "print(cos_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grammer_check(filepath):\n",
        "\n",
        "  tool = language_tool_python.LanguageTool('en-US')\n",
        "  for q in filepath:\n",
        "        q_pd = pd.read_csv(os.path.join(cos_path,q),index_col=0)\n",
        "        # q_pd = q_pd.drop(columns = ['right_ans', 'similarity'])\n",
        "        q_pd['check'] = 0\n",
        "        print(q_pd)\n",
        "        for i in range(len(q_pd)):\n",
        "          text = q_pd.iloc[i][1]\n",
        "          print(text)\n",
        "\n",
        "          # get the matches\n",
        "          matches = tool.check(text)\n",
        "\n",
        "          print(matches)\n",
        "          print(len(matches))\n",
        "          q_pd['check'][i]=len(matches)\n",
        "          q_pd.to_csv(f\"/content/drive/MyDrive/final project/Dataset/output_data/grammar_checker/grammer_checker_{q.split('_')[2].split('.')[0]}.csv\")"
      ],
      "metadata": {
        "id": "jT_Sptl7W58I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiMz6k--5_rM"
      },
      "outputs": [],
      "source": [
        "# def grammer_check(filepath):\n",
        "\n",
        "#   tool = language_tool_python.LanguageTool('en-US')\n",
        "#   for q in filepath:\n",
        "#         q_pd = pd.read_csv(os.path.join(cos_path,q),index_col=0)\n",
        "#         # q_pd = q_pd.drop(columns = ['right_ans', 'similarity'])\n",
        "#         q_pd['check'] = 0\n",
        "#         print(q_pd)\n",
        "#         for i in range(len(q_pd)):\n",
        "#           text = q_pd.iloc[i][1]\n",
        "#           print(text)\n",
        "\n",
        "#           # get the matches\n",
        "#           matches = tool.check(text)\n",
        "\n",
        "#           print(matches)\n",
        "#           print(len(matches))\n",
        "\n",
        "#           my_mistakes = []\n",
        "#           my_corrections = []\n",
        "#           start_positions = []\n",
        "#           end_positions = []\n",
        "\n",
        "#           for rules in matches:\n",
        "#             if len(rules.replacements) > 0:\n",
        "#               start_positions.append(rules.offset)\n",
        "#               end_positions.append(rules.errorLength + rules.offset)\n",
        "#               my_mistakes.append(text[rules.offset: rules.errorLength + rules.offset])\n",
        "#               my_corrections.append(rules.replacements[0])\n",
        "              \n",
        "#               q_pd['check'][i]=len(list(zip(my_mistakes)))\n",
        "#           # my_new_text = list(text)\n",
        "\n",
        "#           # for m in range(len(start_positions)):\n",
        "#           #   for i in range(len(text)):\n",
        "#           #     print(my_new_text,\"-----------\")\n",
        "#           #     print(my_corrections[m])\n",
        "#           #     my_new_text[start_positions[m]] = my_corrections[m]\n",
        "#           #     if (i > start_positions[m] and i < end_positions[m]):\n",
        "#           #       my_new_text[i] = \"\"\n",
        "#           print(list(zip(my_mistakes, my_corrections)))  \n",
        "#           print(len(list(zip(my_mistakes))))\n",
        "#           q_pd.to_csv(f\"/content/drive/MyDrive/final project/Dataset/output_data/grammar_checker/grammer_checker_{q.split('_')[2].split('.')[0]}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammer_check(cos_file)"
      ],
      "metadata": {
        "id": "l91tQ5Vrmqbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzL7VOYM6bme"
      },
      "source": [
        "# Keyword Checker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wai_path = '/content/drive/MyDrive/final project/Dataset/input_data'  # for_valid 폴더 생성  \n",
        "# wai_questions = os.listdir(wai_path)\n",
        "# wai_questions = [p for p in wai_questions if 'csv' in p]\n",
        "# wai_questions  # 컬럼명 질문 내용 답변인 원본 데이터 프레임"
      ],
      "metadata": {
        "id": "VWHX_XcSACEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in wai_questions:\n",
        "    q_pd = pd.read_csv(os.path.join(wai_path,q),index_col=0)\n",
        "    print(q_pd.head(3))\n",
        "\n",
        "    # 컬럼 명 변경하기(문제 번화 확인을 위해 키워드 컬럼명은 나중에 변경)\n",
        "    col_list = list(q_pd.columns)    # 컬럼명 리스트로 변경 후 내용 변경\n",
        "    col_list[0] = 'student_ans'\n",
        "    q_pd.columns = col_list\n",
        "    print(q_pd)\n",
        "\n",
        "    # 문항별 키워드 갯수 확인\n",
        "    keyword=q_pd.columns[1:]\n",
        "    n = len(keyword)\n",
        "    print(n)\n",
        "    \n",
        "    # 문항번호와 키워드 분리\n",
        "    keyword_split = []\n",
        "    for i in range(0,n):\n",
        "      split_key= keyword[i].split('_')[1]\n",
        "      keyword_split.append(split_key)\n",
        "    print(keyword_split)\n",
        "\n",
        "   # 컬럼 명 변경하기\n",
        "    col_list = list(q_pd.columns)    \n",
        "    col_list[1:] = keyword_split          # 분리한 키워드 활용하여 컬럼명 변경/키워드와 동일하게 변경해야 label입력 가능\n",
        "    q_pd.columns = col_list\n",
        "    print(q_pd)\n",
        "    \n",
        "    student_ans = q_pd[q_pd.columns[0]]\n",
        "    print(student_ans)"
      ],
      "metadata": {
        "id": "zjrSq374D8J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_keyword(df,a):\n",
        "    label_encoder = []\n",
        "    label = 0\n",
        "    \n",
        "    for j in student_ans:\n",
        "      if a in str(j):\n",
        "          label = 1\n",
        "      else:\n",
        "          label = 0\n",
        "        # print(label)\n",
        "      label_encoder.append(label)\n",
        "    print(label_encoder)\n",
        "    df[a] = label_encoder\n",
        "    print(q_pd)\n",
        "    q_pd.to_csv(f\"/content/drive/MyDrive/final project/Dataset/output_data/keyword_checker/keyword_checker_{q.split('_')[2].split('.')[0]}.csv\")\n",
        "    return label_encoder\n"
      ],
      "metadata": {
        "id": "ERFZbHOtD78I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,n):\n",
        "  check_keyword(q_pd,keyword_split[i])"
      ],
      "metadata": {
        "id": "lW0ZNctPD75S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def check_keyword(df,a):\n",
        "\n",
        "#   for q in wai_questions:\n",
        "#     q_pd = pd.read_csv(os.path.join(wai_path,q),index_col=0)\n",
        "#     print(q_pd.head(3))\n",
        "\n",
        "#     # 컬럼 명 변경하기(문제 번화 확인을 위해 키워드 컬럼명은 나중에 변경)\n",
        "#     col_list = list(q_pd.columns)    # 컬럼명 리스트로 변경 후 내용 변경\n",
        "#     col_list[0] = 'student_ans'\n",
        "#     q_pd.columns = col_list\n",
        "#     print(q_pd)\n",
        "\n",
        "#     # 문항별 키워드 갯수 확인\n",
        "#     keyword=q_pd.columns[1:]\n",
        "#     n = len(keyword)\n",
        "#     print(n)\n",
        "    \n",
        "#     # 문항번호와 키워드 분리\n",
        "#     keyword_split = []\n",
        "#     for i in range(0,n):\n",
        "#       split_key= keyword[i].split('_')[1]\n",
        "#       keyword_split.append(split_key)\n",
        "\n",
        "#     print(keyword_split)\n",
        "#     # 컬럼 명 변경하기\n",
        "#     col_list = list(q_pd.columns)    \n",
        "#     col_list[1:] = keyword_split          # 분리한 키워드 활용하여 컬럼명 변경/키워드와 동일하게 변경해야 label입력 가능\n",
        "#     q_pd.columns = col_list\n",
        "#     print(q_pd)\n",
        "    \n",
        "#     student_ans = q_pd[q_pd.columns[0]]\n",
        "#     print(student_ans)\n",
        "\n",
        "#     label_encoder = []\n",
        "#     label = 0\n",
        "    \n",
        "#     for j in student_ans:\n",
        "#       if a in str(j):\n",
        "#           label = 1\n",
        "#       else:\n",
        "#           label = 0\n",
        "#         # print(label)\n",
        "#       label_encoder.append(label)\n",
        "#     print(label_encoder)\n",
        "#     df[a] = label_encoder\n",
        "#     print(q_pd)\n",
        "#     q_pd.to_csv(f\"/content/drive/MyDrive/final project/Dataset/output_data/keyword_checker/keyword_checker_{q.split('_')[2].split('.')[0]}.csv\")\n",
        "#     return label_encoder\n"
      ],
      "metadata": {
        "id": "cqDmKmJyZS7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(0,n):\n",
        "#   check_keyword(tr_key,keyword_split[i])"
      ],
      "metadata": {
        "id": "0wTyKEeamsM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlbaeL12jnGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_aAhrEKjShV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}